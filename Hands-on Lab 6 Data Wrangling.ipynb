import pandas as pd
import numpy as np

df = pd.read_csv(r"C:\Users\Nitu Kumar\Downloads\Data Set Assignment\m1_survey_data.csv")

#Find how many duplicate rows exist in the dataframe.
print('There are', df.duplicated().sum(), 'duplicate rows.')
print('There are', df.duplicated('Respondent').sum(), 'duplicate Respondants.')

#Remove the duplicate rows from the dataframe.
df.drop_duplicates(inplace=True)
df.shape

#Verify if duplicates were actually dropped.
print('There are now', df.duplicated().sum(), 'duplicate rows.')
print('There are', df.duplicated('Respondent').sum(), 'duplicate Respondants.')

#Find the missing values for all columns.
with pd.option_context('display.max_rows', None, 'display.max_columns', None):
    print(df.isnull().sum())

#Find out how many rows are missing in the column 'WorkLoc'
print("There are", df['WorkLoc'].isnull().sum(), "missing values in the column 'WorkLoc'")
print("There are", df['EdLevel'].isnull().sum(), "missing values in the column 'EdLevel'")
print("There are", df['Country'].isnull().sum(), "missing values in the column 'Country'")

#Identify the value that is most frequent (majority) in the WorkLoc column.
print(df['ConvertedComp'].mean())
print(df['ConvertedComp'].median())

#Find the value counts for the column WorkLoc.
print('There are', df['WorkLoc'].nunique(), 'unique Work Locations in the survey:')

print('\nWorkLoc                                    value count')      
print('-------                                    -----------')
print(df['WorkLoc'].value_counts())

print('\n\nThere are', df['Employment'].nunique(), 'unique Employment values in the survey:')

print('\nEmployment        value count')      
print('----------        -----------')
print(df['Employment'].value_counts())

print('\n\nThere are', df['UndergradMajor'].nunique(), 'unique UndergradMajor values in the survey:')

print('\nUndergradMajor        value count')      
print('---------------        -----------')
print(df['UndergradMajor'].value_counts())

#Impute (replace) all the empty rows in the column WorkLoc with the value that you have identified as majority.

# your code goes here
import numpy as np

workloc_highest = 'Office'

missing_data = df.isnull()
#print(missing_data.head(5))

print('\nValue counts for missing data in WorkLoc:\n')
print( missing_data['WorkLoc'].value_counts())


print('\nHere are the first 10 rows missing values for WorkLoc:')
print(df[missing_data['WorkLoc']][['Respondent','WorkLoc']].head(10))

df['WorkLoc'].replace(np.nan, workloc_highest, inplace=True)

#After imputation there should ideally not be any empty rows in the WorkLoc column.
#Verify if imputing was successful.
print('\nNew value counts for missing data in WorkLoc:\n')
print(missing_data['WorkLoc'].value_counts())
# No 'True' values means that we have eliminated all empty WorkLoc values.

#Normalizing the data
#List out the various categories in the column 'CompFreq'
print(df['CompFreq'].value_counts())

#Create a new column named 'NormalizedAnnualCompensation'. Use the hint given below if needed.
df.loc[df['CompFreq'] == 'Yearly', 'NormalizedAnnualCompensation']  = 1  * df['CompTotal']
df.loc[df['CompFreq'] == 'Monthly', 'NormalizedAnnualCompensation'] = 12 * df['CompTotal']
df.loc[df['CompFreq'] == 'Weekly', 'NormalizedAnnualCompensation']  = 52 * df['CompTotal']


df[['CompTotal','CompFreq','NormalizedAnnualCompensation']].head(20)

df['NormalizedAnnualCompensation'].median()

